from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from math import pow, sqrt, log
from numbers import Number
import numpy as np
import sys
import random


def compute_accuracy(predict, key):
    positives = 0
    answers = []
    keys = []
    # convert both to lists so we can handle any iterable input
    for item in predict:
        answers.append(item)
    for item in key:
        keys.append(item)

    for index in range(len(answers)):
        if answers[index] == keys[index]:
            positives += 1
    return positives / len(answers)


def calc_entropy(p):
    if p!=0:
        return -p * np.log2(p)
    else:
        return 0


def calc_info_gain(data,classes,feature):
    gain = 0
    nData = len(data)
    # List the values that feature can take
    values = []
    for datapoint in data:
        if datapoint[feature] not in values:
            values.append(datapoint[feature])

    featureCounts = np.zeros(len(values))
    entropy = np.zeros(len(values))
    valueIndex = 0
    # Find where those values appear in data[feature] and the corresponding class
    for value in values:
        dataIndex = 0
        newClasses = []
        for datapoint in data:
            if datapoint[feature] == value:
                featureCounts[valueIndex] += 1
                newClasses.append(classes[dataIndex])
            dataIndex += 1
        # Get the values in newClasses
        classValues = []
        for aclass in newClasses:
            if classValues.count(aclass) == 0:
                classValues.append(aclass)

        classCounts = np.zeros(len(classValues))
        classIndex = 0
        for classValue in classValues:
            for aclass in newClasses:
                if aclass == classValue:
                    classCounts[classIndex] += 1
            classIndex += 1
        for classIndex in range(len(classValues)):
            entropy[valueIndex] += calc_entropy(ﬂoat(classCounts[classIndex]) / sum(classCounts))
        gain += ﬂoat(featureCounts[valueIndex]) / nData * entropy[valueIndex]
        valueIndex += 1
    return gain


def ﬁndPath(graph, start, end, pathSoFar):
    pathSoFar = pathSoFar + [start]
    if start == end:
        return pathSoFar
    if start not in graph:
        return None
    for node in graph[start]:
        if node not in pathSoFar:
            newpath = ﬁndPath(graph, node, end, pathSoFar)
            return newpath
    return None


def make_tree(data,classes,featureNames):

    nData = 0
    nFeatures = 0
    frequency = 0
    totalEntropy = 0
    index = 0

    values = []
    newData = []
    newClasses = []

    default = classes[np.argmax(frequency)]
    if nData == 0 or nFeatures == 0:
        # Have reached an empty branch
        return default
    elif classes.count(classes[0]) == nData:
        # Only 1 class remains
        return classes[0]
    else:
        # Choose which feature is best
        gain = np.zeros(nFeatures)
        for feature in range(nFeatures):
            g = calc_info_gain(data, classes, feature)
            gain[feature] = totalEntropy - g
        bestFeature = np.argmax(gain)
        tree = {featureNames[bestFeature]: {}}
        # Find the possible feature values
        for value in values:
            # Find the datapoints with each feature value
            for datapoint in data:
                if datapoint[bestFeature] == value:
                    if bestFeature == 0:
                        datapoint = datapoint[1:]
                        newNames = featureNames[1:]
                    elif bestFeature == nFeatures:
                        datapoint = datapoint[:-1]
                        newNames = featureNames[:-1]
                    else:
                        datapoint = datapoint[:bestFeature]
                        datapoint.extend(datapoint[bestFeature + 1:])
                        newNames = featureNames[:bestFeature]
                        newNames.extend(featureNames[bestFeature + 1:])
                    newData.append(datapoint)
                    newClasses.append(classes[index])
                index += 1
            # Now recurse to the next level
            subtree = make_tree(newData, newClasses, newNames)
            # And on returning, add the subtree on to the tree
            tree[featureNames[bestFeature]][value] = subtree
        return tree

iris = datasets.load_iris()

# randomize the instances using same random order for data and targets

iris_list = list(zip(iris.data, iris.target))

random.shuffle(iris_list)

iris_data, iris_target = zip(*iris_list)

# split into lists of 70% and 30%
training_num = int(len(iris_data)*.7)
training_set = iris_data[:training_num]
training_set_targets = iris_target[:training_num]
testing_set = iris_data[training_num:]
testing_set_targets = iris_target[training_num:]

atts = [0,1,2,3]
tree = make_tree(training_set,training_set_targets, atts)

results = ﬁndPath(tree, tree[0], tree[len(tree)], "")
# show accuracy results as percent
#print("Iris Hard Coded: " + str(int(compute_accuracy(testing_set_results, iris_target[training_num:]) * 100)) + "%")

